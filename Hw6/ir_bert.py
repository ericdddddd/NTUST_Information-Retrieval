# -*- coding: utf-8 -*-
"""IR-bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EcWkMLWlOnxiuFqZWpJXw9xTwMEOryOI
"""

pip install transformers

import numpy as np
import pandas as pd
import os
import time
import torch

from transformers import BertTokenizer

model_version = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_version)
encoded_input = tokenizer("How old are you?", "I'm 6 years old")
print(encoded_input["input_ids"])

from torch.utils.data import Dataset
 
class BertDataset(Dataset):
    # 讀取前處理後的 tsv 檔並初始化一些參數
    def __init__(self, mode, tokenizer):
        assert mode in ["train", "test", "val"]  # 一般訓練你會需要 dev set
        self.mode = mode
        # 大數據你會需要用 iterator=True
        if mode == 'train' :
            self.positive_df = pd.read_csv("/content/drive/MyDrive/IR/IR/data/Hw6/all_postive.csv").fillna("")
            self.negative_df = pd.read_csv("/content/drive/MyDrive/IR/IR/data/Hw6/BM25Top1000_negative.csv").fillna("")
            self.len = len(self.positive_df)
            self.data = 4
        elif mode == 'val':
            self.val_df = pd.read_csv("/content/drive/MyDrive/IR/IR/data/Hw6/validation_df.csv").fillna("")
            self.len = len(self.val_df)
            self.data = 1
        else:
            self.test_df = pd.read_csv("/content/drive/MyDrive/IR/IR/data/Hw6/test_df.csv").fillna("")
            self.len = len(self.test_df)
            self.data = 1
        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer
    
    # 定義回傳一筆訓練 / 測試數據的函式
    def __getitem__(self, idx):
        if self.mode == "test":
            text_query = self.test_df.iloc[idx, 1]
            text_doc = self.test_df.iloc[idx, 3]
            label_tensor = None
        elif self.mode == "val":
            text_query = self.val_df.iloc[idx, 1]
            text_doc = self.val_df.iloc[idx, 3]
            label_tensor = None
        else:
            if idx > self.len:
              idx = 0
            positive_query = self.positive_df.iloc[idx, 0]
            positive_docs = self.positive_df.iloc[idx, 2]
            #隨機從negative中挑三篇出來
            random_docs = np.random.randint(self.negative_df.shape[0], size=3)
            negative_query = self.negative_df.iloc[random_docs,0].values
            negative_docs = self.negative_df.iloc[random_docs,2].values
            
            # 將 label 文字也轉換成索引方便轉換成 tensor
            positive_doc_insert = idx % 4
            label_tensor = torch.tensor(positive_doc_insert).unsqueeze(0)
            label_tensor = label_tensor.type(torch.LongTensor)
            
            positive_doc = [positive_query,positive_docs]
            negative_doc1 = [negative_query[0],negative_docs[0]]
            negative_doc2 = [negative_query[1],negative_docs[1]]
            negative_doc3 = [negative_query[2],negative_docs[2]]
            bert_input = [negative_doc1,negative_doc2,negative_doc3]
            bert_input.insert(positive_doc_insert,positive_doc)
        
        # test , training 時所需要的資料量不一樣大
        input_ids = torch.zeros([self.data, 512], dtype=torch.long)
        token_type_ids = torch.zeros([self.data, 512], dtype=torch.long)
        attention_mask = torch.zeros([self.data, 512], dtype=torch.long)
        
        if self.mode == 'train':
            encoded_input = tokenizer(bert_input , truncation ='longest_first',return_tensors="pt" ,padding = True)
        else:
            encoded_input = tokenizer(text_query,text_doc, truncation ='longest_first',return_tensors='pt', padding=True)
        
        bert_input_shape = list(encoded_input['input_ids'].size())
        word_size = bert_input_shape[1]
        
        input_ids[:,:word_size] = encoded_input['input_ids']
        token_type_ids[:,:word_size] = encoded_input['token_type_ids']
        attention_mask[:,:word_size] = encoded_input['attention_mask']
        
        #return (encoded_input['input_ids'] ,encoded_input['token_type_ids'] ,encoded_input['attention_mask'], label_tensor)
        if self.mode == 'train':
          return (input_ids,token_type_ids,attention_mask,label_tensor)
          #return (input_ids,attention_mask,label_tensor)
        return(input_ids,token_type_ids,attention_mask)
        #return(input_ids,attention_mask)
    
    def __len__(self):
        return self.len
    
    
# 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞
train_data = BertDataset("train", tokenizer=tokenizer)

print(len(train_data))
data = train_data[0]
print(data[0])
print(data[3])
#data1 = train_data[1]
print(tokenizer.convert_ids_to_tokens(data[0][0]))
#print(tokenizer.convert_ids_to_tokens(data[0][1]))

testset =  BertDataset("test", tokenizer=tokenizer)
validation_set = BertDataset("val", tokenizer=tokenizer)
print(len(testset))
print(len(validation_set))

print(testset[0][0].size())
print(validation_set[0][0].size())

from torch.utils.data import DataLoader
TRAIN_BATCH_SIZE = 3
TEST_BATCH_SIZE = 10
testloader = DataLoader(testset, batch_size = TEST_BATCH_SIZE,drop_last=True,num_workers=2)
validationloader = DataLoader(validation_set, batch_size = TEST_BATCH_SIZE,drop_last=True,num_workers=2)
trainloader = DataLoader(train_data, batch_size = TRAIN_BATCH_SIZE,drop_last=True,num_workers=2,shuffle = True)

print(len(testloader))
print(len(trainloader))
print(len(validationloader))
#print(len(validationloader))

data = next(iter(trainloader))

tokens_tensors = data[0]
segments_tensors = data[1]
masks_tensors = data[2]
label_ids = data[3]
print(f"""
tokens_tensors.shape   = {tokens_tensors.shape} 
{tokens_tensors}
------------------------
segments_tensors.shape = {segments_tensors.shape}
{segments_tensors}
------------------------
masks_tensors.shape    = {masks_tensors.shape}
{masks_tensors}
------------------------
label_ids.shape        = {label_ids.shape}
{label_ids}
""")

data = next(iter(testloader))
print(data[0].size())
print(data[1].size())
print(data[2].size())

from transformers import BertForMultipleChoice

PRETRAINED_MODEL_NAME = "/content/drive/MyDrive/IR/IR/data/Hw6/bert_v2"

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("device:", device)
print(torch.cuda.get_device_name(0))

model = BertForMultipleChoice.from_pretrained(
    PRETRAINED_MODEL_NAME)

model = model.to(device)
# high-level 顯示此模型裡的 modules
print("""
name            module
----------------------""")
for name, module in model.named_children():
    if name == "bert":
        for n, _ in module.named_children():
            print(f"{name}:{n}")
    else:
        print("{:15} {}".format(name, module))
print(model.config)

# 訓練模式
model.train(mode = True)

# 使用 Adam Optim 更新整個分類模型的參數
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)
TRAIN_BATCH_SIZE = 3
EPOCHS = 2
start = time.time()
for epoch in range(EPOCHS):
    s1 = time.time()
    running_loss = 0.0
    for batch_num ,data in enumerate(trainloader):

        if batch_num % 100 == 0:
          print('now batch_num : ' + str(batch_num))
        """
        tokens_tensors, segments_tensors, \
        masks_tensors, labels = [t.to(device) for t in data]
        """
        tokens_tensors = data[0].to(device)
        segments_tensors = data[1].to(device)
        masks_tensors = data[2].to(device)
        labels = data[3].view(TRAIN_BATCH_SIZE).to(device)
        
        # 將參數梯度歸零
        optimizer.zero_grad()
        
        # forward pass
        outputs = model(input_ids=tokens_tensors, 
                        token_type_ids=segments_tensors, 
                        attention_mask=masks_tensors, 
                        labels=labels)

        loss = outputs.loss
        # backward
        loss.backward()
        optimizer.step()
        
        # 紀錄當前 batch loss
        running_loss += loss.item()
        
    print('[epoch %d] loss: %.3f' %
          (epoch + 1, running_loss))
    s2 = time.time()
    print('this epoch costs :' +  str((s2 - s1) / 60) + 'mins')
    
end = time.time()
print('total time :' +  str((end - start) / 60) + 'mins')

model.save_pretrained("/content/drive/MyDrive/IR/IR/data/Hw6/bert_v2")

model.eval()
def get_predictions(model, dataloader):
    
    score = None
    with torch.no_grad():
        # 遍巡整個資料集
        for batch_num ,data in enumerate(dataloader):
            # 將所有 tensors 移到 GPU 上
            if batch_num % 200 == 0:
                print('now batch :' + str(batch_num))
            """
            if next(model.parameters()).is_cuda:
                data = [t.to("cuda:0") for t in data if t is not None]
            """
            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks
            # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱
            tokens_tensors = data[0].to(device)
            segments_tensors = data[1].to(device)
            masks_tensors = data[2].to(device)
            outputs = model(input_ids=tokens_tensors, 
                            token_type_ids=segments_tensors, 
                            attention_mask=masks_tensors)
            
            logits = outputs.logits
            logits = logits.detach().cpu().numpy()
            if score is None:
                score = logits
            else:
                score = np.concatenate((score, logits), axis=None)
            
    return score

# 讓模型跑在 GPU 上並取得訓練集的分類準確率
bert_score = get_predictions(model, validationloader)

pip install ml_metrics

import ml_metrics

bert_score = np.load('/content/drive/MyDrive/IR/IR/data/Hw6/scores/validation_score.npy')
queries = pd.read_csv("/content/drive/MyDrive/IR/IR/data/Hw6/train_20queries.csv").fillna("")
val_data = pd.read_csv("/content/drive/MyDrive/IR/IR/data/Hw6/validation_df.csv").fillna("")
topk = 1000
positive_docs = queries['pos_doc_ids']

alpha_arr = np.arange(0,5,0.1)
result = np.zeros(alpha_arr.shape)

for index,alpha in enumerate(alpha_arr):
  print(alpha)
  sum = 0
  for query_num in range(20):
    res = {}
    BM25_docs = val_data['relevant_docs'][topk * query_num : topk * (query_num + 1)].tolist()
    BM25_score = val_data['BM25_score'][topk * query_num : topk * (query_num + 1)].to_numpy()
    query_positive = positive_docs[query_num].split()
    new_score = BM25_score + alpha * bert_score[topk * query_num : topk * (query_num + 1)]

    for i,j in zip(BM25_docs,new_score):
        res[i] = j
    sorted_x = sorted(res.items(), key=lambda kv: kv[1],reverse = True)
    rescore_docs = []
    for doc in sorted_x:
        rescore_docs.append(doc[0])
    
    score = ml_metrics.mapk(query_positive,rescore_docs,topk)
    sum += score
  #sum /= 20
  result[index] = sum

print(bert_score)
print(result)
print(alpha_arr[np.argmax(result)])

print(len(testloader))
score = get_predictions(model, testloader)

np.save('/content/drive/MyDrive/IR/IR/data/Hw6/scores/validation_score.npy',bert_score)
np.save('/content/drive/MyDrive/IR/IR/data/Hw6/scores/result_score.npy',score)

result_score = np.load('/content/drive/MyDrive/IR/IR/data/Hw6/scores/result_score.npy')
print(result_score.shape)

test_data = pd.read_csv("/content/drive/MyDrive/IR/IR/data/Hw6/test_df.csv").fillna(0)
topk = 1000
alpha = 2.8
result_csv = pd.DataFrame(columns = ["query_id" , "ranked_doc_ids"])
for query_num in range(80):
    res = {}
    relevant_docs = test_data['relevant_docs'][topk * query_num : topk * (query_num + 1)].tolist()
    query_id = test_data['query_num'][query_num * topk].astype('int32')
    query_BM25_score = test_data['BM25_score'][topk * query_num : topk * (query_num + 1)].to_numpy()
    new_score = query_BM25_score + alpha * result_score[topk * query_num : topk * (query_num + 1)]
    for i,j in zip(relevant_docs,new_score):
        res[i] = j
    sorted_x = sorted(res.items(), key=lambda kv: kv[1],reverse = True)
    text = ""
    for doc in sorted_x:
        text += doc[0] + " "
    d = {"query_id": query_id ,"ranked_doc_ids": text}
    this_query_df = pd.DataFrame(data = d ,index=[0])
    result_csv = pd.concat([result_csv , this_query_df] , ignore_index = True)

print(result_csv.shape)

result_csv.to_csv("/content/drive/MyDrive/IR/IR/data/Hw6/result/result_alpha2.88_v2.csv", index=False)